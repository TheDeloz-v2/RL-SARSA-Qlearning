{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc576fcd0dade002",
   "metadata": {},
   "source": [
    "# SARSA & Q-Learning\n",
    "- Francisco Castillo - 21562\n",
    "- Diego Lemus - 21469"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c390ea3e",
   "metadata": {},
   "source": [
    "## Task 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d467cb7a36e433",
   "metadata": {},
   "source": [
    "1. _Defina y explique qué “expected sarsa”?_\n",
    "\n",
    "    a. _¿Cómo se diferencia de “sarsa”?_\n",
    "\n",
    "    SARSA usa la acción muestreada at+1 (la acción realmente tomada) dentro del objetivo.\n",
    "\n",
    "    Expected SARSA reemplaza ese término por la esperanza sobre todas las acciones posibles en el siguiente estado según la política π. Entonces el resultado es que Expected SARSA elimina la variabilidad introducida por la muestra del siguiente action, usando un promedio ponderado por π.\n",
    "    \n",
    "    b. _¿Para qué sirven las modificaciones que se hacen sobre “sarsa”?_\n",
    "\n",
    "    - Reducción de varianza: al usar una expectativa en lugar de una única muestra, disminuye la varianza del objetivo y suele producir actualizaciones más estables.\n",
    "\n",
    "    - Mayor estabilidad con políticas estocásticas: Expected SARSA incorpora explícitamente cómo la política explora en la actualización.\n",
    "\n",
    "    - Mejor comportamiento de convergencia práctica en entornos ruidosos, o sea, menos oscilaciones. Aunque puede costar más computación, ya que hay que sumar sobre todas las acciones.\n",
    "\n",
    "    - Si la política considerada es la greedy, la expectativa coincide con el máximo y Expected SARSA se acerca a Q-learning; con políticas ε-greedy captura la media de comportamiento exploratorio.\n",
    "\n",
    "\n",
    "2. _Defina y explique qué es “n-step TD”_\n",
    "\n",
    "    a. _¿Cómo se diferencia de TD(0)?_\n",
    "\n",
    "    TD(0) es el caso n = 1, es decir solo mira la recompensa siguiente y hace un paso de bootstrapping.\n",
    "\n",
    "    n-step TD usa más recompensas reales antes de bootstrapping, para n grande propaga la información de recompensas más lejos en una sola actualización.\n",
    "    \n",
    "    \n",
    "    b. _¿Cuál es la utilidad de esta modificación?_\n",
    "\n",
    "    - Mejor compromiso sesgo-varianza: n pequeño conlleva más sesgo por bootstrapping pero baja varianza; n grande conlleva menos sesgo, pero mayor varianza. Ajustando n se obtiene un trade-off útil.\n",
    "\n",
    "    - Propagación más rápida de las recompensas: con n>1 la señal de recompensa puede actualizar estados anteriores más directamente que TD(0).\n",
    "\n",
    "    - Flexibilidad: permite adaptar el algoritmo a la naturaleza del problema, como rutas largas o recompensas retardadas.\n",
    "    \n",
    "    \n",
    "    c. _¿Qué usa como objetivo?_\n",
    "\n",
    "    Usa el n-step return Gt(n)​ que es la suma de las n recompensas descontadas más un término de bootstrapping con el valor estimado en S(t+n).\n",
    "\n",
    "3. _¿Cuál es la diferencia entre SARSA y Q-learning?_\n",
    "\n",
    "    - On-policy vs Off-policy:\n",
    "        \n",
    "        SARSA es on-policy: actualiza Q usando la acción que la política realmente toma en el siguiente estado, es decir, la política bajo la cual se actúa.\n",
    "\n",
    "        Q-learning es off-policy: actualiza Q usando el máximo sobre acciones en el siguiente estado (el mejor comportamiento posible), independientemente de la acción realmente tomada por la política exploratoria.\n",
    "\n",
    "    - Fórmula del objetivo:\n",
    "\n",
    "        SARSA: el objetivo es la acción muestreada.\n",
    "\n",
    "        Q-learning: el objetivo es el max sobre acciones.\n",
    "\n",
    "    - Consecuencia práctica:\n",
    "\n",
    "        SARSA aprende el valor de la política seguida (por ejemplo ε-greedy que explora). Si la política explora, SARSA aprende teniendo en cuenta ese comportamiento, por ende es más \"cauto\" en presencia de riesgo.\n",
    "\n",
    "        Q-learning aprende el valor de la política óptima (la greedy) independientemente de la exploración; por eso puede ser más agresivo y en ciertos entornos. Q-learning puede aprender una política que asume que nunca caerá por explorar, resultando en comportamiento arriesgado cuando en la práctica explora.\n",
    "\n",
    "\n",
    "    - Convergencia\n",
    "        \n",
    "        Ambos convergen bajo condiciones adecuadas (visitar pares estado-acción infinitas veces, tasas de aprendizaje adecuadas), pero las garantías formales y la velocidad práctica dependen del setting (exploración, función de aproximación, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaf033da5d7e9e7",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fece0e",
   "metadata": {},
   "source": [
    "### Análisis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
